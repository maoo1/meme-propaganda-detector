{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maoo1/meme-propaganda-detector/blob/master/MEME_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[sem eval task](https://propaganda.math.unipd.it/semeval2024task4/teampage.php?passcode=7a4c50dc60f44593a07529d2253593e9)\n",
        "\n",
        "to do:\n",
        "- import models\n",
        "- finetune models\n",
        "- evaluate models"
      ],
      "metadata": {
        "id": "yGGoy1eh7d6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull origin master"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pufhlcvU5ZCo",
        "outputId": "b955e318-6103-4ddf-ce45-1e02d7a76f58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -am \"data processing complete, working on finishing up the training loop\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0Oasvl2TALh",
        "outputId": "fc7af607-4a56-456c-fa45-13c85c08e4ee"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git push origin master"
      ],
      "metadata": {
        "id": "uKNeFdJZmh6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Setting up libraries and mounting google drive\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ARK95eVKc77d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuVfr1Ia-Rwz",
        "outputId": "ba7857b5-9219-433b-b6b5-56b78bbb1c62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Collecting gdown==v4.6.3\n",
            "  Downloading gdown-4.6.3-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown==v4.6.3) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown==v4.6.3) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown==v4.6.3) (1.17.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown==v4.6.3) (4.12.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==v4.6.3) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==v4.6.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==v4.6.3) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==v4.6.3) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown==v4.6.3) (2.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown==v4.6.3) (1.7.1)\n",
            "Downloading gdown-4.6.3-py3-none-any.whl (14 kB)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, gdown, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 5.2.0\n",
            "    Uninstalling gdown-5.2.0:\n",
            "      Successfully uninstalled gdown-5.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 gdown-4.6.3 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.26.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.10)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.3\n"
          ]
        }
      ],
      "source": [
        "# install dependencies and codebase\n",
        "# transformers Hugging Face library: access to BERT / GPT\n",
        "!pip install torch transformers datasets tqdm gdown==v4.6.3\n",
        "!pip install evaluate\n",
        "#!pip install sklearn_hierarchical_classification\n",
        "!mkdir checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "n5qoqEgkALxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mounting data and files to drive\n",
        "# hypothetically can upload SemEval datasets to drive for easier access + we would both have access\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZGjFcsyBNOt",
        "outputId": "1cd9d585-540b-429b-a94d-b189790148c5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HIERARCHY"
      ],
      "metadata": {
        "id": "aGHsqJPbeUBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing the Data"
      ],
      "metadata": {
        "id": "grW6aACJiSt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple, Union\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "rif5mVhOiW9M"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating Meme object and dataset"
      ],
      "metadata": {
        "id": "DAAcbDU_CP9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# represents single meme as text, label\n",
        "@dataclass\n",
        "class Meme:\n",
        "  text: str\n",
        "  labels: Union[List[str], None] # labels can be none for test data\n",
        "\n",
        "  @staticmethod\n",
        "  def from_dict(data: dict):\n",
        "    text = data[\"text\"]\n",
        "    labels = data.get(\"labels\")\n",
        "    if len(labels) == 0:\n",
        "        labels = ['None']\n",
        "    return Meme(text=text, labels=labels)\n",
        "\n",
        "# custom dataset for meme data, supporting multi-label classification\n",
        "class MemeDataset(Dataset):\n",
        "\n",
        "  def __init__(self, tokenizer, data: List[Dict], label_encoder):\n",
        "    MemeDataset.tokenizer = tokenizer\n",
        "    self.examples = [Meme.from_dict(item) for item in data]\n",
        "    self.label_encoder = label_encoder\n",
        "\n",
        "    if label_encoder:\n",
        "      # encode labels if available\n",
        "\n",
        "      self.encoded_labels = [label_encoder.fit_transform([item.labels]) for item in self.examples]\n",
        "    else:\n",
        "      self.encoded_labels = None\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.examples)\n",
        "\n",
        "  # def __getitem__(self, idx):\n",
        "    # return self.examples[idx]\n",
        "  def __getitem__(self, idx):\n",
        "    example = self.examples[idx]\n",
        "    labels = self.encoded_labels[idx] if self.encoded_labels else None\n",
        "    return {\"text\": example.text, \"labels\": labels}\n",
        "\n",
        "  # batch processing\n",
        "  @staticmethod\n",
        "  def collate_fn(batched_samples: List[Meme], max_length=512):\n",
        "\n",
        "    batched_texts = [sample.text for sample in batched_samples]\n",
        "    batched_labels = [sample.labels for sample in batched_samples if sample.labels is not None]\n",
        "\n",
        "    text_encoding = MemeDataset.tokenizer(\n",
        "        batched_texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    if batched_labels:\n",
        "      labels_tensor = torch.tensor(batched_labels, dtype=torch.float32) # multi-label as float\n",
        "    else:\n",
        "      labels_tensor = None\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": text_encoding[\"input_ids\"],\n",
        "        \"attention_mask\": text_encoding[\"attention_mask\"],\n",
        "        \"labels\": labels_tensor,\n",
        "    }\n",
        "\n",
        "  def inverse_transform_labels(encoded_labels, label_encoder):\n",
        "\n",
        "    if encoded_labels is not None:\n",
        "      return label_encoder.inverse_transform(encoded_labels)\n",
        "    return []"
      ],
      "metadata": {
        "id": "lWnMg3fp3vlX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "class_labels = ['Appeal to authority', 'Appeal to fear/prejudice', 'Bandwagon',\n",
        "                'Black-and-white Fallacy/Dictatorship', 'Causal Oversimplification',\n",
        "                'Doubt', 'Exaggeration/Minimisation', 'Flag-waving',\n",
        "                'Glittering generalities (Virtue)', 'Loaded Language',\n",
        "                \"Misrepresentation of Someone's Position (Straw Man)\",\n",
        "                'Name calling/Labeling', 'Obfuscation, Intentional vagueness, Confusion',\n",
        "                'Presenting Irrelevant Data (Red Herring)', 'Reductio ad hitlerum',\n",
        "                'Repetition', 'Slogans', 'Smears', 'Thought-terminating cliché',\n",
        "                'Whataboutism', 'None']\n",
        "mlb = MultiLabelBinarizer(classes=class_labels)"
      ],
      "metadata": {
        "id": "1aC7GpnMCA8-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_json(file_path):\n",
        "  with open(file_path, 'r') as f:\n",
        "    return json.load(f)\n",
        "\n",
        "# initializing datasets\n",
        "def initialize_datasets(tokenizer, dataset_paths: Dict, label_encoder) -> dict:\n",
        "\n",
        "  raw_datasets = {}\n",
        "\n",
        "  # loading raw datasets from file paths\n",
        "  for split_name, file_path in dataset_paths.items():\n",
        "    raw_datasets[split_name] = load_json(file_path)\n",
        "\n",
        "  # fit mlb with full hierarchy\n",
        "  #all_labels = list(hierarchy.keys())\n",
        "  #label_encoder = MultiLabelBinarizer(classes=all_labels)\n",
        "  #label_encoder.fit([all_labels])\n",
        "  #print(type(label_encoder))\n",
        "\n",
        "  # convert raw datasets to MemeDataset objects\n",
        "  split_datasets = {}\n",
        "  for split_name, split_data in raw_datasets.items():\n",
        "    split_datasets[split_name] = MemeDataset(tokenizer, list(split_data), label_encoder)\n",
        "\n",
        "  return split_datasets\n",
        "\n",
        "dataset_paths = {\n",
        "  \"train\": '/content/drive/MyDrive/NLP_FINAL/train.json',\n",
        "  \"validation\": '/content/drive/MyDrive/NLP_FINAL/validation.json',\n",
        "  # \"dev_unlabeled\":  '/content/drive/MyDrive/NLP_FINAL/dev_unlabeled.json',\n",
        "  \"test\":  '/content/drive/MyDrive/NLP_FINAL/dev_subtask1_en.json'\n",
        "}"
      ],
      "metadata": {
        "id": "XMZXudsGjYgj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the data"
      ],
      "metadata": {
        "id": "LgpnzmXXojiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "\"\"\"\n",
        "datasets = {}\n",
        "for split_name in raw_datasets.keys():\n",
        "  split_data = list(raw_datasets[split_name])\n",
        "\n",
        "  datasets[split_name] = MemeDataset(tokenizer, split_data)\n",
        "\"\"\"\n",
        "# loading datasets\n",
        "datasets = initialize_datasets(tokenizer, dataset_paths, mlb)\n",
        "\n",
        "length_train = len(datasets['train'])\n",
        "length_val = len(datasets['validation'])\n",
        "# length_dev_unlabaled = len(datasets['dev_unlabeled'])\n",
        "length_test = len(datasets['test'])\n",
        "\n",
        "print(\"num training examples:\", length_train)\n",
        "print(\"num validation examples:\", length_val)\n",
        "print(\"num test examples:\", length_test)\n",
        "\n",
        "#validation_dataloader = DataLoader(datasets['validation'],\n",
        "#                                   batch_size=64,\n",
        "#                                   shuffle=False,\n",
        "#                                   collate_fn=MemeDataset.collate_fn,\n",
        "#                                   num_workers=2)\n",
        "\n",
        "for i in range(0, 6):\n",
        "  print(datasets['train'][i]['text'])\n",
        "  print(datasets['train'][i]['labels'])\n",
        "  print(mlb.inverse_transform(datasets['train'][i]['labels']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XW-Sqh2TIx98",
        "outputId": "ccae3741-6a02-4c9a-bd17-059d685d6a21"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num training examples: 7000\n",
            "num validation examples: 500\n",
            "num test examples: 1000\n",
            "THIS IS WHY YOU NEED\\n\\nA SHARPIE WITH YOU AT ALL TIMES\n",
            "[[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "[('Black-and-white Fallacy/Dictatorship',)]\n",
            "GOOD NEWS!\\n\\nNAZANIN ZAGHARI-RATCLIFFE AND ANOOSHEH ASHOORI HAVE BEEN RELEASED\\n\\nAfter years of being unjustly detained in Iran, they are making their way safely back to the UK.\n",
            "[[0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "[('Glittering generalities (Virtue)', 'Loaded Language')]\n",
            "PAING PHYO MIN IS FREE!\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
            "[('None',)]\n",
            "Move your ships away!\\n\\noooook\\n\\nMove your ships away!\\n\\nNo, and I just added 10 more\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
            "[('None',)]\n",
            "WHEN YOU'RE THE FBI, THEY LET YOU DO IT.\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]]\n",
            "[('Thought-terminating cliché',)]\n",
            "PUTIN'S SECRET CAMOUFLAGE ARMY\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
            "[('None',)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking if Preprocessing Data Correct"
      ],
      "metadata": {
        "id": "vWH760D_Fn94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_label_encoding(dataset, mlb):\n",
        "    \"\"\"\n",
        "    Checks if all items in the dataset have been properly encoded by MLB.\n",
        "    \"\"\"\n",
        "    for i, item in enumerate(dataset.examples):\n",
        "        if item.labels is not None:\n",
        "            # Encode labels\n",
        "            encoded = mlb.transform([item.labels])[0]\n",
        "            # Convert to 2D numpy array for inverse_transform\n",
        "            decoded = mlb.inverse_transform(np.array([encoded]))[0]\n",
        "\n",
        "            # Check that original and decoded labels match\n",
        "            assert set(item.labels) == set(decoded), \\\n",
        "                f\"Mismatch in encoding for sample {i}.\\nOriginal: {item.labels}\\nDecoded: {decoded}\"\n",
        "\n",
        "    print(f\"All labels in {len(dataset.examples)} examples are properly encoded!\")\n",
        "\n",
        "check_label_encoding(datasets['train'], mlb)\n",
        "check_label_encoding(datasets['validation'], mlb)\n",
        "check_label_encoding(datasets['test'], mlb)"
      ],
      "metadata": {
        "id": "eh2astDBc-Qu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d17efd02-fbf0-408f-8460-074c909ee11a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All labels in 7000 examples are properly encoded!\n",
            "All labels in 500 examples are properly encoded!\n",
            "All labels in 1000 examples are properly encoded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAINING"
      ],
      "metadata": {
        "id": "10SuxDLJHPYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(datasets['train'],\n",
        "                               batch_size=64,\n",
        "                               shuffle=True,\n",
        "                               collate_fn=MemeDataset.collate_fn,\n",
        "                               num_workers=2)\n",
        "\n",
        "validation_dataloader = DataLoader(datasets['validation'],\n",
        "                                   batch_size=64,\n",
        "                                   shuffle=False,\n",
        "                                   collate_fn=MemeDataset.collate_fn,\n",
        "                                   num_workers=2)\n",
        "# final eval only\n",
        "test_dataloader = DataLoader(datasets['test'],\n",
        "                              batch_size=64,\n",
        "                              shuffle=False,\n",
        "                              collate_fn=MemeDataset.collate_fn,\n",
        "                              num_workers=2)"
      ],
      "metadata": {
        "id": "H1nbxfEwHFbu"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Optimizer, AdamW\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# models bertweet + roberta\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=20, torch_dtype=\"auto\")\n",
        "\n",
        "bertweet = AutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\")\n",
        "training_args = TrainingArguments(output_dir=\"test_trainer\", eval_strategy=\"epoch\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "\n",
        "    logits, labels = eval_pred\n",
        "    # convert logits to binary predictions\n",
        "    predictions = (logits > 0.5).astype(int)\n",
        "\n",
        "    # convert predictions and labels to lists of labels for each example\n",
        "    pred_labels = {\n",
        "        idx: [mlb.inverse_transform(pred_row)]# for i, pred in enumerate(pred_row) if pred == 1]\n",
        "        for idx, pred_row in enumerate(predictions)\n",
        "    }\n",
        "\n",
        "    #gold_labels = {\n",
        "    #    idx: [label_map[i] for i, gold in enumerate(label_row) if gold == 1]\n",
        "    #    for idx, label_row in enumerate(labels)\n",
        "    #}\n",
        "\n",
        "    # evaluating hierarchical metrics\n",
        "    #h_precision, h_recall, h_f1 = evaluate_h(pred_labels, gold_labels)\n",
        "\n",
        "    #return {\n",
        "    #    \"h_precision\": h_precision,\n",
        "    #    \"h_recall\": h_recall,\n",
        "    #    \"h_f1\": h_f1\n",
        "    #}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=bertweet,\n",
        "    args=training_args,\n",
        "    train_dataset=datasets['train'],\n",
        "    eval_dataset=datasets['validation'],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "vpnHMKBLHnst"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}